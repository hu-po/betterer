{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Automatic Differentiation in JAX\n",
    "\n",
    "https://jax.readthedocs.io/en/latest/jax-101/04-advanced-autodiff.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**3 + 2*x**2 - 3*x + 1\n",
    "dfdx = jax.grad(f)\n",
    "d2fdx2 = jax.grad(dfdx)\n",
    "d3fdx3 = jax.grad(d2fdx2)\n",
    "d4fdx4 = jax.grad(d3fdx3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(f):\n",
    "    \"\"\" Hessian is second-order derivative (e.g. Jacobian of the gradient)\"\"\"\n",
    "    # return jax.jacrev(jax.grad(f))\n",
    "    return jax.jacfwd(jax.grad(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: jnp.dot(x, x)\n",
    "\n",
    "x = jnp.array([1, 2, 3]).astype(jnp.float32)\n",
    "hessian(f)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAML Loss function\n",
    "def loss(params, data):\n",
    "    \"\"\" MSE. \"\"\"\n",
    "    return jnp.mean((data[0] - data[1])**2)\n",
    "\n",
    "def meta_loss(params, data, lr=1e-5):\n",
    "    \"\"\" Meta loss of loss\"\"\"\n",
    "    grads = jax.grad(loss)(params, data)\n",
    "    return loss(params - lr*grads, data)\n",
    "\n",
    "params, data = None, None\n",
    "meta_grads = jax.grad(meta_loss)(params, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theta is parameters of the value function model\n",
    "value_fn = lambda theta, state: jnp.dot(theta, state)\n",
    "theta = jnp.array([0.1, -0.1, 0.])\n",
    "\n",
    "# State at time t-1\n",
    "s_tm1 = jnp.array([1., 2., -1.])\n",
    "# State at time t\n",
    "s_t = jnp.array([2., 1., 0.])\n",
    "# Reward\n",
    "r_t = jnp.array(1.)\n",
    "\n",
    "def td_loss(theta, s_tm1, r_t, s_t):\n",
    "    # temporal difference as loss for value function\n",
    "    v_tm1 = value_fn(theta, s_tm1)\n",
    "    target = r_t + value_fn(theta, s_t)\n",
    "    # stop gradients from getting to value function through s_t\n",
    "    target = jax.lax.stop_gradient(target)\n",
    "    return (target - v_tm1) ** 2\n",
    "\n",
    "def update(theta, s_tm1, r_t, s_t, lr=1e-3):\n",
    "    grads = jax.grad(td_loss)(theta, s_tm1, r_t, s_t)\n",
    "    return theta - lr*grads, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not apply to batch dimmension\n",
    "per_example_grads = jax.vmap(jax.grad(td_loss), in_axes=(None, 0, 0, 0))\n",
    "\n",
    "batch_of_s_tm1 = jnp.stack([s_tm1, s_tm1])\n",
    "print(f's_tm1 {s_tm1.shape}, batched {batch_of_s_tm1.shape}')\n",
    "batch_of_s_t = jnp.stack([s_t, s_t])\n",
    "batch_of_r_t = jnp.stack([r_t, r_t])\n",
    "\n",
    "per_example_grads(theta, batch_of_s_tm1, batch_of_r_t, batch_of_s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtd_lossdtheta = jax.grad(td_loss)\n",
    "dtd_lossdtheta(theta, s_tm1, r_t, s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_jit = per_example_grads\n",
    "with_jit = jax.jit(per_example_grads)\n",
    "\n",
    "%timeit no_jit(theta, batch_of_s_tm1, batch_of_r_t, batch_of_s_t)\n",
    "%timeit with_jit(theta, batch_of_s_tm1, batch_of_r_t, batch_of_s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use stop_gradient to shield the auto-diff from\n",
    "# non-differentiable functions\n",
    "\n",
    "def f(x):\n",
    "    # non-differentiable\n",
    "    return jnp.round(x)\n",
    "\n",
    "def straight_through_f(x):\n",
    "    return x - jax.lax.stop_gradient(x) + jax.lax.stop_gradient(f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
