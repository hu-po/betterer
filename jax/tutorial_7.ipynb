{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel Evaluation in JAX\n",
    "https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import List, Set, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img: jnp.DeviceArray = jnp.arange(5)\n",
    "kernel: jnp.DeviceArray = jnp.array([0.1, 0.9, 0.1])\n",
    "\n",
    "def convolve(\n",
    "    x: jnp.DeviceArray,\n",
    "    kernel: jnp.DeviceArray,\n",
    "    ) -> jnp.DeviceArray:\n",
    "    \"\"\" Convolution operation hardcoded to 3x1 kernel. \"\"\"\n",
    "    out: List = []\n",
    "    for i in range(1, len(x)-1):\n",
    "        out.append(jnp.dot(x[i-1:i+2], kernel))\n",
    "    return jnp.array(out)\n",
    "\n",
    "convolve(img, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_devices = jax.local_device_count()\n",
    "img_p: jnp.DeviceArray = jnp.stack([img] * num_devices)\n",
    "kernel_p: jnp.DeviceArray = jnp.stack([kernel] * num_devices)\n",
    "print(f'img_p {img_p}')\n",
    "print(f'kernel_p {kernel_p}')\n",
    "print(f'img {img}')\n",
    "print(f'kernel {kernel}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pmap is comparable to vmap because both transformations map a function over array axes,\n",
    "but where vmap vectorizes functions by pushing the mapped axis down into primitive operations\n",
    "pmap instead replicates the function and executes each replica on its own XLA device in parallel.\n",
    "'''\n",
    "jax.vmap(convolve)(img_p, kernel_p)\n",
    "jax.pmap(convolve)(img_p, kernel_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Tuple, NamedTuple\n",
    "\n",
    "LEARNING_RATE = 5e-3\n",
    "\n",
    "class Params(NamedTuple):\n",
    "    w: jnp.ndarray\n",
    "    b: jnp.ndarray\n",
    "\n",
    "def init(rng) -> Params:\n",
    "    \"\"\" Initialize parameters for Linear Regression. \"\"\"\n",
    "    w_key, b_key = jax.random.split(rng)\n",
    "    return Params(\n",
    "        w=jax.random.normal(w_key, ()),\n",
    "        b=jax.random.normal(b_key, ()),\n",
    "    )\n",
    "\n",
    "def loss_fn(params: Params, xs: jnp.ndarray, ys: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\" MSE Loss. \"\"\"\n",
    "    pred: jnp.ndarray = params.w * xs + params.b\n",
    "    return jnp.mean((pred - ys) ** 2)\n",
    "\n",
    "# \"name\" the axis on which we want to parallel map accross devices\n",
    "@functools.partial(jax.pmap, axis_name='num_devices')\n",
    "def update(params: Params, xs: jnp.ndarray, ys: jnp.ndarray) -> Tuple[Params, jnp.ndarray]:\n",
    "    \"\"\" One gradient descent step. \"\"\"\n",
    "\n",
    "    # Performed on each device individually\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, xs, ys)\n",
    "\n",
    "    # Performed accross devices\n",
    "    mean_grads = jax.lax.pmean(grads, axis_name='num_devices')\n",
    "    mean_loss = jax.lax.pmean(loss, axis_name='num_devices')\n",
    "\n",
    "    # Performed on each device individually\n",
    "    update_fn = lambda param, g: param - g * LEARNING_RATE\n",
    "    new_params = jax.tree_multimap(update_fn, params, grads)\n",
    "    \n",
    "    return new_params, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fake noisy linear data\n",
    "rng = jax.random.PRNGKey(42)\n",
    "true_params: Params = Params(w=2, b=-1)\n",
    "xs: jnp.ndarray = jax.random.normal(rng, (128, 1))\n",
    "noise: jnp.ndarray = 0.5 * jax.random.normal(rng, (128, 1))\n",
    "ys: jnp.ndarray = xs * true_params.w + noise + true_params.b\n",
    "\n",
    "# Initialize our Linear Regression params, replicated across devices\n",
    "rng = jax.random.PRNGKey(1)\n",
    "params: Params = init(rng)\n",
    "num_devices = jax.local_device_count()\n",
    "replicated_params = jax.tree_map(\n",
    "    lambda x: jnp.array([x] * num_devices),\n",
    "    params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(arr: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\" Split first axis of arr evenly accross num_devices. \"\"\"\n",
    "    return arr.reshape(num_devices, arr.shape[0] // num_devices, *arr.shape[1:])\n",
    "\n",
    "# Split the training data into num_device chunks\n",
    "x_split = split(xs)\n",
    "y_split = split(ys)\n",
    "print(f'xs {xs.shape}')\n",
    "print(f'x_split {x_split.shape}')\n",
    "print(f'ys {ys.shape}')\n",
    "print(f'y_split {y_split.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1000\n",
    "\n",
    "print(f' Starting training loop')\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    replicated_params, loss = update(replicated_params, x_split, y_split)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'\\t epoch {epoch:3d} loss {loss[0]:.3f}')\n",
    "\n",
    "params = jax.device_get(jax.tree_map(lambda x: x[0], replicated_params))\n",
    "print(f' true params y = {true_params.w:.3f} * x + {true_params.b:.3f} ')\n",
    "print(f' pred params y = {params.w:.3f} * x + {params.b:.3f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
