{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Kaggle with ChatGPT and CoPilot\n",
    "\n",
    "All the code here was generated (as much as possible, but some human edits were made)\n",
    "\n",
    "https://www.kaggle.com/competitions/spaceship-titanic/\n",
    "https://chat.openai.com/chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set default styling for plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Import scikit-learn for machine learning\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames for train and test csv files\n",
    "train_file = 'train.csv'\n",
    "test_file = 'test.csv'\n",
    "\n",
    "# Read the CSV file into a dataframe, using the first row as the column names\n",
    "df_train_raw = pd.read_csv(train_file, header=0)\n",
    "df_test_raw = pd.read_csv(test_file, header=0)\n",
    "\n",
    "# Print the first few rows of the dataframe\n",
    "print(\"Training data:\")\n",
    "print(df_train_raw.head())\n",
    "\n",
    "print(\"Test data:\")\n",
    "print(df_test_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary of the dataframe, including the data type of each column\n",
    "print(df_train_raw.info())\n",
    "\n",
    "# Print a summary of the dataframe, including the data type of each column\n",
    "print(df_test_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one-hot encoding of the 'col' column\n",
    "homeplanet_onehot = pd.get_dummies(df_train_raw['HomePlanet'])\n",
    "print(homeplanet_onehot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in [\n",
    "    \"HomePlanet\",\n",
    "    \"CryoSleep\",\n",
    "    \"Cabin\",\n",
    "    \"Destination\",\n",
    "    \"VIP\",\n",
    "    \"Name\",\n",
    "]:\n",
    "    print(f\"All unique values in {col_name}: {df_train_raw[col_name].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "df_train_filtered = df_train_raw.dropna()\n",
    "df_test_filtered = df_test_raw.dropna()\n",
    "\n",
    "# Get the number of rows and columns in the dataframe\n",
    "num_rows, _ = df_train_filtered.shape\n",
    "num_rows_test, _ = df_test_filtered.shape\n",
    "\n",
    "# Print the differenc in rows between the original and filtered dataframes\n",
    "print(f\"Number of rows removed (TRAIN): {df_train_raw.shape[0] - num_rows}\")\n",
    "print(f\"Number of rows removed (TEST): {df_test_raw.shape[0] - num_rows_test}\")\n",
    "\n",
    "# Print the percentage of rows removed\n",
    "print(f\"Percentage of rows removed (TRAIN): {(df_train_raw.shape[0] - num_rows) / df_train_raw.shape[0] * 100:.2f}%\")\n",
    "print(f\"Percentage of rows removed (TEST): {(df_test_raw.shape[0] - num_rows_test) / df_test_raw.shape[0] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that will be used for training\n",
    "training_cols_names = [\n",
    "    \"HomePlanet\",\n",
    "    \"CryoSleep\",\n",
    "    # \"Cabin\",\n",
    "    \"Destination\",\n",
    "    \"Age\",\n",
    "    \"VIP\",\n",
    "    \"RoomService\",\n",
    "    \"FoodCourt\",\n",
    "    \"ShoppingMall\",\n",
    "    \"Spa\",\n",
    "    \"VRDeck\",\n",
    "    \"Transported\",\n",
    "]\n",
    "\n",
    "# Create a new dataframe with only the training_cols_names columns\n",
    "df_train_filtered_cols = df_train_filtered.filter(items=training_cols_names)\n",
    "df_test_filtered_cols = df_test_filtered.filter(items=training_cols_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns we want to one-hot encode\n",
    "onehot_cols = [\n",
    "    \"HomePlanet\",\n",
    "    \"CryoSleep\",\n",
    "    # \"Cabin\",\n",
    "    \"Destination\",\n",
    "    \"VIP\",\n",
    "]\n",
    "\n",
    "# Create one-hot encodings of the specified columns\n",
    "df_train_filtered_cols_onehot = pd.get_dummies(\n",
    "    df_train_filtered_cols, columns=onehot_cols + [\"Transported\"])\n",
    "df_test_filtered_cols_onehot = pd.get_dummies(\n",
    "    df_test_filtered_cols, columns=onehot_cols)\n",
    "\n",
    "print(df_train_filtered_cols_onehot.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Select the columns to normalize\n",
    "cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n",
    "\n",
    "# Normalize the selected columns\n",
    "df_train_filtered_cols_onehot[cols] = scaler.fit_transform(df_train_filtered_cols_onehot[cols])\n",
    "df_test_filtered_cols_onehot[cols] = scaler.fit_transform(df_test_filtered_cols_onehot[cols])\n",
    "\n",
    "print(df_train_filtered_cols_onehot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train data shape:')\n",
    "print(df_train_filtered_cols_onehot.shape)\n",
    "print('Test data shape:')\n",
    "print(df_test_filtered_cols_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove the \"Transported_True\" and \"Transported_False\" columns\n",
    "train_df = df_train_filtered_cols_onehot.drop([\"Transported_True\", \"Transported_False\"], axis=1)\n",
    "\n",
    "# Get the list of column names\n",
    "features = train_df.columns\n",
    "\n",
    "# Select the target columns\n",
    "target = [\"Transported_True\", \"Transported_False\"]\n",
    "\n",
    "# Create an MLPClassifier object\n",
    "clf = MLPClassifier()\n",
    "\n",
    "# Set the maximum number of iterations to 1000\n",
    "clf.set_params(\n",
    "    # Hyperparameters chosen by ChatGPT\n",
    "    max_iter=500,\n",
    "    batch_size=256,\n",
    "    # learning_rate=0.001,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.2,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Print a message before starting the training process\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Fit the classifier using the features and target from the dataframe\n",
    "clf.fit(train_df[features], df_train_filtered_cols_onehot[target])\n",
    "\n",
    "# Print a message after the training process is complete\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained classifier to make predictions on the test dataset\n",
    "predictions = clf.predict(df_test_filtered_cols_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns of the first dataframe as a set\n",
    "cols1 = set(df_test_filtered_cols_onehot.columns)\n",
    "\n",
    "# Get the columns of the second dataframe as a set\n",
    "cols2 = set(train_df.columns)\n",
    "\n",
    "# Get the columns that are different between the two dataframes\n",
    "diff_cols = cols1.symmetric_difference(cols2)\n",
    "\n",
    "# Print the columns that are different\n",
    "print(diff_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test_filtered.head())\n",
    "print(df_test_raw.head())\n",
    "\n",
    "print(df_test_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe\n",
    "submissions_filtered = pd.DataFrame()\n",
    "\n",
    "# Add the \"PassengerId\" column from the \"df_test_filtered\" dataframe\n",
    "submissions_filtered[\"PassengerId\"] = df_test_filtered[\"PassengerId\"]\n",
    "\n",
    "# Add the first column from the \"predictions\" dataframe\n",
    "submissions_filtered[\"Transported_True\"] = predictions[:, 0]\n",
    "\n",
    "# Add the second column from the \"predictions\" dataframe\n",
    "submissions_filtered[\"Transported_False\"] = predictions[:, 1]\n",
    "\n",
    "# Print the resulting dataframe\n",
    "print(submissions_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the \"Transported_True\" column from the \"predictions\" dataframe\n",
    "# and convert it to a boolean column called \"Transported\"\n",
    "submissions_filtered[\"Transported\"] = predictions[:, 0].astype(bool)\n",
    "\n",
    "# Print the resulting dataframe\n",
    "print(submissions_filtered.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe\n",
    "submission = submissions_filtered[[\"PassengerId\", \"Transported\"]]\n",
    "\n",
    "# Get the rows of the \"PassengerId\" column from the \"df_test_raw\" dataframe\n",
    "# that are not in the \"submissions_filtered\" dataframe\n",
    "missing_rows = df_test_raw[~df_test_raw[\"PassengerId\"].isin(submissions_filtered[\"PassengerId\"])]\n",
    "\n",
    "# Create new rows in the \"submissions_filtered\" dataframe for each missing row\n",
    "# and set the \"Transported\" value to False\n",
    "submission = submission.append(\n",
    "    missing_rows[[\"PassengerId\"]].assign(Transported=False),\n",
    "    ignore_index=True,\n",
    "    sort=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test_raw.shape)\n",
    "print(submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dataframe to a CSV file with the column headers included\n",
    "submission.to_csv('file_with_headers.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('spacetitanic')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43b6a310364049d71a1be735cbc14c5e7868bf68955f2327973c4d05a23cb995"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
