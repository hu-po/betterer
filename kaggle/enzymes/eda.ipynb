{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enzyme Stability Predition on Kaggle with ChatGPT\n",
    "\n",
    "https://www.kaggle.com/competitions/novozymes-enzyme-stability-prediction/\n",
    "\n",
    "https://chat.openai.com/chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train.csv file into a dataframe called \"train_df\"\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# read the test.csv file into a dataframe called \"test_df\"\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 5 rows of the train_df dataframe\n",
    "print(train_df.head())\n",
    "\n",
    "# print the first 5 rows of the test_df dataframe\n",
    "print(test_df.head())\n",
    "\n",
    "# print the datatypes of each column in the train_df dataframe\n",
    "print(train_df.dtypes)\n",
    "\n",
    "# print the datatypes of each column in the test_df dataframe\n",
    "print(test_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the percentage of rows with null or NaN values\n",
    "train_null_percentage = train_df.isnull().sum() / train_df.shape[0] * 100\n",
    "test_null_percentage = test_df.isnull().sum() / test_df.shape[0] * 100\n",
    "\n",
    "# print the result\n",
    "print(f\"Train null percentage: {train_null_percentage}\")\n",
    "print(f\"Test null percentage: {test_null_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any rows that contain NaN or null values\n",
    "train_df_filtered = train_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write train_df_filtered to a new csv file called \"train_filtered.csv\"\n",
    "train_df_filtered.to_csv(\"train_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the number of rows in the dataframe\n",
    "# TODO: remove this line before submitting for final training\n",
    "train_df_filtered = train_df_filtered.head(6)\n",
    "test_df = test_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the train and test dataframes for normalization\n",
    "combined_df = pd.concat([train_df_filtered, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a histogram of the \"pH\" column\n",
    "combined_df[\"pH\"].plot.hist()\n",
    "\n",
    "# add a title and x-axis label\n",
    "plt.title(\"pH\")\n",
    "plt.xlabel(\"pH\")\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a histogram of the \"tm\" column\n",
    "train_df_filtered[\"tm\"].plot.hist()\n",
    "\n",
    "# add a title and x-axis label\n",
    "plt.title(\"tm\")\n",
    "plt.xlabel(\"tm\")\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the \"pH\" column\n",
    "ph_mean = combined_df[\"pH\"].mean()\n",
    "ph_std = combined_df[\"pH\"].std()\n",
    "test_df[\"pH\"] = (test_df[\"pH\"] - ph_mean) / ph_std\n",
    "train_df_filtered[\"pH\"] = (train_df_filtered[\"pH\"] - ph_mean) / ph_std\n",
    "\n",
    "# Write test_df to a new csv file called \"test_normalized.csv\"\n",
    "test_df.to_csv(\"test_normalized.csv\", index=False)\n",
    "\n",
    "# Normalize the \"tm\" column\n",
    "tm_mean = train_df_filtered[\"tm\"].mean()\n",
    "tm_std = train_df_filtered[\"tm\"].std()\n",
    "train_df_filtered[\"tm\"] = (train_df_filtered[\"tm\"] - tm_mean) / tm_std\n",
    "\n",
    "# Print tm_mean and tm_std\n",
    "print(f\"tm_mean: {tm_mean}\")\n",
    "print(f\"tm_std: {tm_std}\")\n",
    "\n",
    "\n",
    "# Write train_df_filtered to a new csv file called \"train_normalized.csv\"\n",
    "train_df_filtered.to_csv(\"train_normalized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a histogram of the \"tm\" column\n",
    "train_df_filtered[\"tm\"].plot.hist()\n",
    "\n",
    "# add a title and x-axis label\n",
    "plt.title(\"tm\")\n",
    "plt.xlabel(\"tm\")\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a histogram of the \"pH\" column\n",
    "train_df_filtered[\"pH\"].plot.hist()\n",
    "\n",
    "# add a title and x-axis label\n",
    "plt.title(\"pH\")\n",
    "plt.xlabel(\"pH\")\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a histogram of the \"pH\" column\n",
    "test_df[\"pH\"].plot.hist()\n",
    "\n",
    "# add a title and x-axis label\n",
    "plt.title(\"pH\")\n",
    "plt.xlabel(\"pH\")\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSource to Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the unique values in the \"data_source\" column\n",
    "data_sources = combined_df[\"data_source\"].unique()\n",
    "\n",
    "# print the result\n",
    "print(data_sources)\n",
    "\n",
    "# print the length of the result\n",
    "print(len(data_sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one-hot encoding of the \"data_source\" column\n",
    "one_hot_encoding = pd.get_dummies(df[\"data_source\"])\n",
    "\n",
    "# Join the one-hot encoding with the original dataframe\n",
    "df_with_one_hot = df.join(one_hot_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding of Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForPreTraining.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "# Clear out any stale data in the GPU\n",
    "torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "\n",
    "# Encode  the \"data_source\" column in the dataframe\n",
    "with torch.no_grad():\n",
    "    train_df_filtered[\"data_source_encoded_BERT\"] = train_df_filtered[\"data_source\"].apply(lambda x: model(**tokenizer(x, return_tensors=\"pt\")).prediction_logits.cpu().numpy().flatten())\n",
    "    test_df[\"data_source_encoded_BERT\"] = test_df[\"data_source\"].apply(lambda x: model(**tokenizer(x, return_tensors=\"pt\")).prediction_logits.cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_filtered[\"data_source_encoded_BERT\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding of Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "\n",
    "# Clear out any stale data in the GPU\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load the ESM-2 model\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "# Big Boy model\n",
    "# model, alphaber = esm.pretrained.esm2_t48_15B_UR50D()\n",
    "model.eval()\n",
    "\n",
    "def encode_sequence(sequence):\n",
    "    \"\"\"Encode a sequence using the ESM-2 model.\"\"\"\n",
    "\n",
    "    # Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter([('0', sequence)])\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "    # Extract per-residue representations (on CPU)\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=True)    \n",
    "    token_representations = results[\"representations\"][33]\n",
    "\n",
    "    # Generate per-sequence representations via averaging\n",
    "    # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "    sequence_representations = []\n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "\n",
    "    encoded_sequence = sequence_representations[0].cpu().numpy()\n",
    "\n",
    "    print(f\"Converted {sequence} into {encoded_sequence}\")\n",
    "    return encoded_sequence\n",
    "\n",
    "# encode the \"protein_sequence\" column in the train_df dataframe into a new column\n",
    "train_df_filtered[\"esm2_t33_650M_UR50D_sequence\"] = train_df_filtered[\"protein_sequence\"].apply(encode_sequence)\n",
    "test_df[\"esm2_t33_650M_UR50D_sequence\"] = test_df[\"protein_sequence\"].apply(encode_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output columns\n",
    "col = \"esm2_t33_650M_UR50D_sequence\"\n",
    "\n",
    "# Find the length of the arrays in the column\n",
    "array_length = train_df_filtered[col].apply(len).max()\n",
    "\n",
    "# Generate the output column names\n",
    "esm2_output_cols = [\"esm2_t33_650M_UR50D_sequence_{}\".format(i) for i in range(array_length)]\n",
    "\n",
    "# Convert the column to multiple columns containing single numbers\n",
    "train_df_filtered[esm2_output_cols] = pd.DataFrame(train_df_filtered[col].tolist(), index=train_df_filtered.index)\n",
    "test_df[esm2_output_cols] = pd.DataFrame(test_df[col].tolist(), index=test_df.index)\n",
    "\n",
    "\n",
    "print(train_df_filtered[esm2_output_cols].head())\n",
    "print(test_df[esm2_output_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output columns\n",
    "col = \"data_source_encoded_BERT\"\n",
    "\n",
    "# Find the length of the arrays in the column\n",
    "array_length = train_df_filtered[col].apply(len).max()\n",
    "\n",
    "# Generate the output column names\n",
    "bert_output_cols = [\"data_source_encoded_BERT_{}\".format(i) for i in range(array_length)]\n",
    "\n",
    "# Convert the column to multiple columns containing single numbers\n",
    "train_df_filtered[bert_output_cols] = pd.DataFrame(train_df_filtered[col].tolist(), index=train_df_filtered.index)\n",
    "test_df[bert_output_cols] = pd.DataFrame(test_df[col].tolist(), index=test_df.index)\n",
    "\n",
    "print(train_df_filtered[bert_output_cols].head())\n",
    "print(test_df[bert_output_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Define the input and target columns\n",
    "# X = train_df_filtered[[\"esm2_t33_650M_UR50D_sequence\", \"data_source_encoded_BERT\", \"pH\"]]\n",
    "# X = train_df_filtered[bert_output_cols + esm2_output_cols + [\"pH\"]]\n",
    "# X = train_df_filtered[bert_output_cols + esm2_output_cols]\n",
    "X = train_df_filtered[esm2_output_cols]\n",
    "y = train_df_filtered[\"tm\"]\n",
    "\n",
    "# Train the regression model\n",
    "reg = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input columns\n",
    "# X = test_df[[\"esm2_t33_650M_UR50D_sequence\", \"data_source_encoded_BERT\", \"pH\"]]\n",
    "X = test_df[esm2_output_cols]\n",
    "\n",
    "# Use the trained \"reg\" model to make predictions on the \"tm\" column\n",
    "tm_predictions = reg.predict(X)\n",
    "\n",
    "# Un-normalize the predictions\n",
    "tm_predictions = tm_predictions * tm_std + tm_mean\n",
    "\n",
    "# Add the predictions to a new column in the dataframe\n",
    "test_df[\"tm\"] = tm_predictions\n",
    "\n",
    "print(test_df[['seq_id', 'tm']].head())\n",
    "\n",
    "# Write the dataframe to the output file\n",
    "test_df[['seq_id', 'tm']].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Train Ready CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [\n",
    "    'train_filtered',\n",
    "    'test',\n",
    "]:\n",
    "    for embedding_model in [\n",
    "        'embeddings_esm2_t33_650M_UR50D',\n",
    "        # 'embeddings_esm1v_t33_650M_UR90S_1',\n",
    "    ]:\n",
    "        # Read in the two CSV files\n",
    "        df1 = pd.read_csv(f\"{dataset}_{embedding_model}.csv\")\n",
    "        df2 = pd.read_csv(f\"{dataset}_normalized.csv\")\n",
    "\n",
    "        # Join the two dataframes on the \"seq_id\" column\n",
    "        df_merged = df1.merge(df2, on=\"seq_id\")\n",
    "\n",
    "        # Get only the columns we will use for training\n",
    "        if dataset == 'test':\n",
    "            df_subset = df_merged[[\"seq_id\", \"pH\"] + [f\"latent_{i}\" for i in range(1280)]]\n",
    "        else:\n",
    "            df_subset = df_merged[[\"seq_id\", \"tm\", \"pH\"] + [f\"latent_{i}\" for i in range(1280)]]\n",
    "\n",
    "        # Print the merged dataframe\n",
    "        print(df_subset.head(5))\n",
    "\n",
    "        # Write the df_subset dataframe to a CSV file called \"train_ready_embeddings_esm2_t33_650M_UR50D.csv\"\n",
    "        df_subset.to_csv(f\"{dataset}_ready_{embedding_model}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
